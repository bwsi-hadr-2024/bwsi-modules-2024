{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Civil Air Patrol and Georegistration\n",
    "\n",
    "In the previous session, we saw how a series of 2D images taken of a 3D scene can be used to recover the 3D information, by exploiting geometric constraints of the cameras. Now the question is, how do we take this technique and apply it in a disaster response scenario?\n",
    "\n",
    "We are going to look at a specific case study, using images from the Low Altitude Disaster Imagery (LADI) dataset, taken by the Civil Air Patrol (CAP). As we work with this dataset, keep in mind the two major questions from the previous lecture:\n",
    "\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Civil Air Patrol\n",
    "Civil Air Patrol (CAP) is the civilian auxiliary of the United States Air Force (USAF). The origins of CAP date back to the pre-World War II era. As the Axis powers became a growing threat to the world, civilian aviators in the United States feared that the government would shut down general aviation as a precautionary measure. These aviators thus had to prove to the federal government that civilian aviation was not only not a danger, but actually a benefit to the war effort. \n",
    "\n",
    "As a result of these efforts, two separate programs were created. One was a Civilian Pilot Training Program, intended to increase the available people that could operate an aircraft should the need to deploy additional troops arise. The second actually called for the organization of civilian aviators and opened the door to the creation of CAP. \n",
    "\n",
    "Once the United States entered WWII proper, CAP began to embark a plethora of activities, some of which are still practiced today. They continued to do cadet education programs. They also began patrolling the coasts and borders. Finally, they started in 1942 conducting search and rescue (SAR) missions. These missions were a resounding success, and one of the main components of CAP today.\n",
    "\n",
    "CAP has five congressionally mandated missions:\n",
    "\n",
    "(1) To provide an organization toâ€”\n",
    "(A) encourage and aid citizens of the United States in contributing their efforts, services, and resources in developing aviation and in maintaining air supremacy; and\n",
    "(B) encourage and develop by example the voluntary contribution of private citizens to the public welfare.\n",
    "\n",
    "(2) To provide aviation education and training especially to its senior and cadet members.\n",
    "\n",
    "(3) To encourage and foster civil aviation in local communities.\n",
    "\n",
    "(4) To provide an organization of private citizens with adequate facilities to assist in meeting local and national emergencies.\n",
    "\n",
    "(5) To assist the Department of the Air Force in fulfilling its noncombat programs and missions.\n",
    "\n",
    "source: https://www.law.cornell.edu/uscode/text/36/40302\n",
    "\n",
    "CAP's main series of missions revolve around emergency response. CAP is involved in roughly 85% of all SAR missions in the United States and its territories. After natural disasters, CAP is responsible for assessing damage in affected communities, delivering supplies, providing transportation, in addition to its usual SAR missions. \n",
    "\n",
    "https://kvia.com/health/2020/06/18/el-paso-civil-air-patrol-flying-virus-tests-to-labs-in-money-saving-effort-for-texas/\n",
    "\n",
    "https://www.southernminn.com/article_2c5739a5-826f-53bb-a658-922fb1aa1627.html\n",
    "\n",
    "Part of their emergency programming is taking aerial imagery of affected areas. This imagery is the highest resolution, most timely imagery that we have available of a post-disaster situation. Even the highest resolution satellite imagery is often either limited in their geographical coverage, not very timely or occluded by clouds. These are images taken of Puerto Rico after Hurricane Maria in 2017.\n",
    "\n",
    "<img src=\"notebook_images/A0008AP-_932ec345-75a9-4005-9879-da06ba0af37e.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-52_e71b5e09-ec3c-4ea6-8ac8-b9d1e4b714cb.jpg\" width=\"500\"  />\n",
    "\n",
    "<img src=\"notebook_images/A0016-54_f5273b60-dec4-4617-8f01-d67f16001dcb.jpg\" width=\"500\"  />\n",
    "\n",
    "CAP has taken hundreds of thousands of images of disaster-affected areas in the past decades. And yet, even though it is some of the best imagery we have access to, it is rarely if ever used in practice. _Why?_\n",
    "\n",
    "## The LADI dataset\n",
    "Part of the effort in making CAP imagery more useful is trying to make more sense of the content of the images. To that end, researchers at MIT Lincoln Laboratory released the Low Altitude Disaster Imagery (LADI) dataset. This dataset contains hundreds of thousands of CAP images that have crowdsourced labels corresponding to infrastructure, environment and damage categories. This begins to answer the first of the two questions we set out initially. We'll start working on these labels tomorrow. For now, we will solely focus on the images themselves.\n",
    "\n",
    "<img src=\"notebook_images/labels.png\" width=\"500\"  />\n",
    "\n",
    "What are some of the limitations of this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Imagine you have acquired $200,000 to implement some improvement to the way CAP takes aerial imagery. Hurricane season starts in five months, so whatever improvements need to be implemented by then. Separate into your breakout rooms and answer the following questions:\n",
    "- What specific hurdles to using CAP images do you want to address? Identify at least two.\n",
    "- Design a proposal to address the challenges you identified above, taking into account the budget and time constraints. Improvements can be of any sort (technical, political, social, etc).\n",
    "- What are the advantages and disadvantages of implementing your proposal?\n",
    "- Identify at least three different stakeholder groups in this situation. What are their specific needs? How does your proposal address these needs? How does your proposal fall short?\n",
    "- Draw out a budget breakdown and a timeline, as well as a breakdown of which stakeholders you are prioritizing and why. Prepare to present these at 1:30pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Reconstruction in a Real World Reference Frame\n",
    "\n",
    "If we want to answer the second of our two guiding questions, we must be able to make a translation between where something is in the image and its location in real world coordinates. Let's stake stock of what tools we have thus far. We spent a good amount of time discussing structure from motion as a way to reconstruct a 3D scene from 2D images. Recall the limitations of this approach:\n",
    "- There need to be more than one image in a sequence.\n",
    "- Sequential images need to have enough overlap that there are common features.\n",
    "- At least one pair of sequential images must have sufficient translation such that the problem is not ill-posed.\n",
    "- The reconstruction is given in an arbitrary reference frame up to scale. \n",
    "\n",
    "What does that last point mean? The arbitrary reference part refers to the fact that the origin and the axes are aligned with the first camera. The up to scale part means that all distances are preserved up to a factor $\\lambda$. Therefore the scene retains the general shape, but the size of the scene is not conserved. Without additional information, it is impossible to know how the reconstructed scene relates to any other reference frame, and translating the reconstruction to real world coordinates is impossible.\n",
    "\n",
    "However, recall that we do typically have at least a coarse estimate of the camera's GPS coordinates, therefore we have estimates of the distances between sequential cameras. Consider a reconstruction of just two images. Then a good estimate of $\\lambda$ is:\n",
    "\n",
    "$\\lambda = \\frac{D_{GPS}}{D_{reconstruction}}$\n",
    "\n",
    "This is slightly more complicated for more than two images. Typically, a solver will initialize the camera positions at their GPS coordinates and use bundle adjustment to correct the errors in the GPS measurements, although certainly there's more than one way to do this.\n",
    "\n",
    "Let's give this a shot and see what happens! As it so happens, OpenSfM is already equipped to handle GPS coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import open3d as o3d\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: opensfm_main.py [-h] command ...\n",
      "\n",
      "positional arguments:\n",
      "  command               Command to run\n",
      "    extract_metadata    Extract metadata from images' EXIF tag\n",
      "    detect_features     Compute features for all images\n",
      "    match_features      Match features between image pairs\n",
      "    create_rig          Create rig by creating `rig_cameras.json` and\n",
      "                        `rig_assignments.json` files.\n",
      "    create_tracks       Link matches pair-wise matches into tracks\n",
      "    reconstruct         Compute the reconstruction\n",
      "    reconstruct_from_prior\n",
      "                        Reconstruct from prior reconstruction\n",
      "    bundle              Bundle a reconstruction\n",
      "    mesh                Add delaunay meshes to the reconstruction\n",
      "    undistort           Save radially undistorted images\n",
      "    compute_depthmaps   Compute depthmap\n",
      "    compute_statistics  Compute statistics and save them in the stats folder\n",
      "    export_ply          Export reconstruction to PLY format\n",
      "    export_openmvs      Export reconstruction to openMVS format\n",
      "    export_visualsfm    Export reconstruction to NVM_V3 format from VisualSfM\n",
      "    export_pmvs         Export reconstruction to PMVS\n",
      "    export_bundler      Export reconstruction to bundler format\n",
      "    export_colmap       Export reconstruction to colmap format\n",
      "    export_geocoords    Export reconstructions in geographic coordinates\n",
      "    export_report       Export a nice report based on previously generated\n",
      "                        statistics\n",
      "    extend_reconstruction\n",
      "                        Extend a reconstruction\n",
      "    create_submodels    Split the dataset into smaller submodels\n",
      "    align_submodels     Align submodel reconstructions\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 17:02:10,465 INFO: Loading existing EXIF for image_url_pr_10_13_sample_12.jpg\n",
      "2024-07-05 17:02:10,482 INFO: Loading existing EXIF for image_url_pr_10_13_sample_11.jpg\n",
      "2024-07-05 17:02:10,483 INFO: Loading existing EXIF for image_url_pr_10_13_sample_13.jpg\n",
      "2024-07-05 17:02:10,483 INFO: Loading existing EXIF for image_url_pr_10_13_sample_07.jpg\n",
      "2024-07-05 17:02:10,483 INFO: Loading existing EXIF for image_url_pr_10_13_sample_08.jpg\n",
      "2024-07-05 17:02:12,084 INFO: Planning to use 12700.800000000001 MB of RAM for both processing queue and parallel processing.\n",
      "2024-07-05 17:02:12,084 INFO: Scale-space expected size of a single image : 23.8125 MB\n",
      "2024-07-05 17:02:12,084 INFO: Expecting to queue at most 123 images while parallel processing of 1 images.\n",
      "2024-07-05 17:02:12,084 INFO: Reading data for image image_url_pr_10_13_sample_12.jpg (queue-size=0)\n",
      "2024-07-05 17:02:12,504 INFO: Finished reading images\n",
      "2024-07-05 17:02:12,504 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_12.jpg\n",
      "2024-07-05 17:02:12,505 INFO: Reading data for image image_url_pr_10_13_sample_11.jpg (queue-size=0)\n",
      "2024-07-05 17:02:12,911 INFO: Finished reading images\n",
      "2024-07-05 17:02:12,911 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_11.jpg\n",
      "2024-07-05 17:02:12,912 INFO: Reading data for image image_url_pr_10_13_sample_13.jpg (queue-size=0)\n",
      "2024-07-05 17:02:13,313 INFO: Finished reading images\n",
      "2024-07-05 17:02:13,314 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_13.jpg\n",
      "2024-07-05 17:02:13,314 INFO: Reading data for image image_url_pr_10_13_sample_07.jpg (queue-size=0)\n",
      "2024-07-05 17:02:13,718 INFO: Finished reading images\n",
      "2024-07-05 17:02:13,718 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_07.jpg\n",
      "2024-07-05 17:02:13,719 INFO: Reading data for image image_url_pr_10_13_sample_08.jpg (queue-size=0)\n",
      "2024-07-05 17:02:14,119 INFO: Finished reading images\n",
      "2024-07-05 17:02:14,119 INFO: Skip recomputing ROOT_HAHOG features for image image_url_pr_10_13_sample_08.jpg\n",
      "2024-07-05 17:02:15,713 INFO: Matching 6 image pairs\n",
      "2024-07-05 17:02:15,718 INFO: Computing pair matching with 1 processes\n",
      "2024-07-05 17:02:15,732 DEBUG: No segmentation for image_url_pr_10_13_sample_07.jpg, no features masked.\n",
      "2024-07-05 17:02:15,745 DEBUG: No segmentation for image_url_pr_10_13_sample_08.jpg, no features masked.\n",
      "2024-07-05 17:02:16,346 DEBUG: Matching image_url_pr_10_13_sample_07.jpg and image_url_pr_10_13_sample_08.jpg.  Matcher: FLANN (symmetric) T-desc: 0.623 T-robust: 0.002 T-total: 0.626 Matches: 724 Robust: 709 Success: True\n",
      "2024-07-05 17:02:16,358 DEBUG: No segmentation for image_url_pr_10_13_sample_11.jpg, no features masked.\n",
      "2024-07-05 17:02:16,724 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_07.jpg.  Matcher: FLANN (symmetric) T-desc: 0.378 Matches: FAILED\n",
      "2024-07-05 17:02:16,902 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_08.jpg.  Matcher: FLANN (symmetric) T-desc: 0.177 Matches: FAILED\n",
      "2024-07-05 17:02:16,913 DEBUG: No segmentation for image_url_pr_10_13_sample_13.jpg, no features masked.\n",
      "2024-07-05 17:02:16,926 DEBUG: No segmentation for image_url_pr_10_13_sample_12.jpg, no features masked.\n",
      "2024-07-05 17:02:17,464 DEBUG: Matching image_url_pr_10_13_sample_13.jpg and image_url_pr_10_13_sample_12.jpg.  Matcher: FLANN (symmetric) T-desc: 0.556 T-robust: 0.004 T-total: 0.562 Matches: 1659 Robust: 1650 Success: True\n",
      "2024-07-05 17:02:17,640 DEBUG: Matching image_url_pr_10_13_sample_13.jpg and image_url_pr_10_13_sample_11.jpg.  Matcher: FLANN (symmetric) T-desc: 0.174 T-robust: 0.001 T-total: 0.176 Matches: 513 Robust: 486 Success: True\n",
      "2024-07-05 17:02:17,822 DEBUG: Matching image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_12.jpg.  Matcher: FLANN (symmetric) T-desc: 0.175 T-robust: 0.004 T-total: 0.181 Matches: 1681 Robust: 1646 Success: True\n",
      "2024-07-05 17:02:17,822 INFO: Matched 6 pairs (perspective-perspective: 6) in 2.108841079998456 seconds (0.351473609999933 seconds/pair).\n",
      "2024-07-05 17:02:19,475 INFO: reading features\n",
      "2024-07-05 17:02:19,527 DEBUG: Merging features onto tracks\n",
      "2024-07-05 17:02:19,567 DEBUG: Good tracks: 3316\n",
      "2024-07-05 17:02:21,317 INFO: Starting incremental reconstruction\n",
      "2024-07-05 17:02:21,363 INFO: Starting reconstruction with image_url_pr_10_13_sample_12.jpg and image_url_pr_10_13_sample_13.jpg\n",
      "2024-07-05 17:02:21,380 INFO: Two-view 5-points reconstruction inliers (transposed=False): 1663 / 1664\n",
      "2024-07-05 17:02:21,414 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 17:02:21,414 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 17:02:21,414 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 17:02:21,622 INFO: Triangulated: 1514\n",
      "2024-07-05 17:02:21,858 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 17:02:21,858 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 17:02:21,858 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 17:02:22,309 INFO: Ran GLOBAL bundle in 0.43 secs.with 2/1514/3028 (2.00) shots/points/proj. (avg. length)\n",
      "2024-07-05 17:02:22,310 DEBUG: Ceres Solver Report: Iterations: 50, Initial cost: 4.728458e+01, Final cost: 1.804233e+01, Termination: CONVERGENCE\n",
      "2024-07-05 17:02:22,323 INFO: Removed outliers: 0\n",
      "2024-07-05 17:02:22,345 INFO: -------------------------------------------------------\n",
      "2024-07-05 17:02:22,359 INFO: image_url_pr_10_13_sample_11.jpg resection inliers: 686 / 686\n",
      "2024-07-05 17:02:22,371 INFO: Adding image_url_pr_10_13_sample_11.jpg to the reconstruction\n",
      "2024-07-05 17:02:22,487 INFO: Re-triangulating\n",
      "2024-07-05 17:02:22,488 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 17:02:22,488 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 17:02:22,489 INFO: GPS constraints X (3) - Xp (3)\n",
      "2024-07-05 17:02:23,570 INFO: Ran GLOBAL bundle in 1.05 secs.with 3/2339/5439 (2.33) shots/points/proj. (avg. length)\n",
      "2024-07-05 17:02:23,571 DEBUG: Ceres Solver Report: Iterations: 70, Initial cost: 1.285972e+02, Final cost: 4.944933e+01, Termination: CONVERGENCE\n",
      "2024-07-05 17:02:24,085 INFO: Ran GLOBAL bundle in 0.22 secs.with 3/2437/5644 (2.32) shots/points/proj. (avg. length)\n",
      "2024-07-05 17:02:24,085 DEBUG: Ceres Solver Report: Iterations: 13, Initial cost: 5.151640e+01, Final cost: 5.097435e+01, Termination: CONVERGENCE\n",
      "2024-07-05 17:02:24,107 INFO: Removed outliers: 0\n",
      "2024-07-05 17:02:24,107 INFO: Reconstruction now has 3 shots.\n",
      "2024-07-05 17:02:24,110 INFO: -------------------------------------------------------\n",
      "2024-07-05 17:02:24,113 INFO: Some images can not be added\n",
      "2024-07-05 17:02:24,113 INFO: -------------------------------------------------------\n",
      "2024-07-05 17:02:24,115 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 17:02:24,115 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 17:02:24,115 INFO: GPS constraints X (3) - Xp (3)\n",
      "2024-07-05 17:02:24,665 INFO: Ran GLOBAL bundle in 0.52 secs.with 3/2437/5644 (2.32) shots/points/proj. (avg. length)\n",
      "2024-07-05 17:02:24,665 DEBUG: Ceres Solver Report: Iterations: 31, Initial cost: 5.939108e+01, Final cost: 5.139650e+01, Termination: CONVERGENCE\n",
      "2024-07-05 17:02:24,687 INFO: Removed outliers: 0\n",
      "2024-07-05 17:02:24,718 INFO: Starting reconstruction with image_url_pr_10_13_sample_07.jpg and image_url_pr_10_13_sample_08.jpg\n",
      "2024-07-05 17:02:24,795 INFO: Two-view 5-points reconstruction inliers (transposed=False): 709 / 709\n",
      "2024-07-05 17:02:24,812 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 17:02:24,812 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 17:02:24,812 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 17:02:24,898 INFO: Triangulated: 709\n",
      "2024-07-05 17:02:24,998 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 17:02:24,998 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 17:02:24,999 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 17:02:25,124 INFO: Ran GLOBAL bundle in 0.11 secs.with 2/709/1418 (2.00) shots/points/proj. (avg. length)\n",
      "2024-07-05 17:02:25,124 DEBUG: Ceres Solver Report: Iterations: 29, Initial cost: 2.685710e+01, Final cost: 1.123582e+01, Termination: CONVERGENCE\n",
      "2024-07-05 17:02:25,131 INFO: Removed outliers: 0\n",
      "2024-07-05 17:02:25,140 INFO: -------------------------------------------------------\n",
      "2024-07-05 17:02:25,141 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 17:02:25,142 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 17:02:25,142 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 17:02:25,214 INFO: Ran GLOBAL bundle in 0.06 secs.with 2/709/1418 (2.00) shots/points/proj. (avg. length)\n",
      "2024-07-05 17:02:25,214 DEBUG: Ceres Solver Report: Iterations: 11, Initial cost: 1.782806e+01, Final cost: 1.123583e+01, Termination: CONVERGENCE\n",
      "2024-07-05 17:02:25,222 INFO: Removed outliers: 0\n",
      "2024-07-05 17:02:25,232 INFO: Reconstruction 0: 3 images, 2437 points\n",
      "2024-07-05 17:02:25,233 INFO: Reconstruction 1: 2 images, 709 points\n",
      "2024-07-05 17:02:25,233 INFO: 2 partial reconstructions in total.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /opt/conda/bin/activate opensfm\n",
    "export PATH=$PATH:/opt/utils/OpenSfM/bin\n",
    "export MPLBACKEND=gtk3agg # if running within a notebook\n",
    "opensfm -h\n",
    "\n",
    "\n",
    "\n",
    "# Take initial guess of intrinsic parameters through metadata\n",
    "opensfm extract_metadata CAP_sample_1\n",
    "opensfm detect_features CAP_sample_1\n",
    "opensfm match_features CAP_sample_1\n",
    "opensfm create_tracks CAP_sample_1\n",
    "opensfm reconstruct CAP_sample_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/conda/bin/activate opensfm\n",
    "export PATH=$PATH:/opt/utils/OpenSfM/bin\n",
    "export MPLBACKEND=gtk3agg # if running within a notebook\n",
    "\n",
    "opensfm export_ply CAP_sample_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read point cloud data\n",
    "pcd = o3d.io.read_point_cloud(\"CAP_sample_1/reconstruction.ply\", format='ply')\n",
    "\n",
    "# convert to array\n",
    "points = np.asarray(pcd.points)\n",
    "colors = np.asarray(pcd.colors)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter3d(\n",
    "            x=points[:,0], y=points[:,1], z=points[:,2], \n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color=colors)\n",
    "        )\n",
    "    ],\n",
    "    layout=dict(\n",
    "        scene=dict(\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.write_html('test.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we seeing? We see two collections of points, both mostly coplanar internally (which we expect, given that this is a mostly planar scene), but the two sets are not aligned with each other! Let's look a bit more closely..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here, we're just going to plot the z (altitude) values of the reconstructed points\n",
    "point_coord = np.asarray(pcd.points)\n",
    "plt.hist(point_coord[:, 2].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not only are the points misaligned, but we're getting wild altitude values! **What's going on?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let's make a critical assumption: all of the image coordinates (the GPS coordinates of the camera as it takes an image) all lie on a plane (in the mathematical sense). Answer the following questions:\n",
    "- How many points are needed to specify a (mathematical) plane?\n",
    "- In addition to the number of points, what other requirement do those points need?\n",
    "- Look at the visualization above. Do the camera points fulfill that requirement?\n",
    "- One way to resolve the ambiguity is to determine what direction is \"up\" (i.e. pointing away from the center of the Earth). Propose a solution to determine the up-vector. You can either assume the same setup that we currently have or propose new sensors/other setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>CLICK HERE TO SEE THE PROPOSED SOLUTION</summary>\n",
    "    We're going to make a fair (but limited) assumption that the ground is mostly flat. It turns out we can fit a plane through the reconstructed ground points and find a direction perpendicular to the plane (called the plane normal). If the ground is flat, then the normal should be close enough to the up direction. Note that this assumption does not hold for an area with a lot of inclination. In practice, we would most likely augment this with a Digital Elevation Model (DEM)\n",
    "\n",
    "</details>\n",
    "\n",
    "<img src=\"notebook_images/plane_normal.png\" width=\"500\"  />\n",
    "\n",
    "To implement the proposed solution, we can go to the CAP_sample_1/config.yaml file and modify \"align_orientation_prior\" from \"horizontal\" to \"plane_based\". Afterwards, we run the previous commands as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 16:55:41,806 INFO: reading features\n",
      "2024-07-05 16:55:41,857 DEBUG: Merging features onto tracks\n",
      "2024-07-05 16:55:41,897 DEBUG: Good tracks: 3305\n",
      "2024-07-05 16:55:43,649 INFO: Starting incremental reconstruction\n",
      "2024-07-05 16:55:43,693 INFO: Starting reconstruction with image_url_pr_10_13_sample_11.jpg and image_url_pr_10_13_sample_12.jpg\n",
      "2024-07-05 16:55:43,852 INFO: Two-view 5-points reconstruction inliers (transposed=False): 1730 / 1730\n",
      "2024-07-05 16:55:43,888 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 16:55:43,888 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 16:55:43,888 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 16:55:44,103 INFO: Triangulated: 1609\n",
      "2024-07-05 16:55:44,331 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 16:55:44,331 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 16:55:44,331 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 16:55:44,964 INFO: Ran GLOBAL bundle in 0.61 secs.with 2/1609/3218 (2.00) shots/points/proj. (avg. length)\n",
      "2024-07-05 16:55:44,964 DEBUG: Ceres Solver Report: Iterations: 69, Initial cost: 3.110935e+01, Final cost: 1.800401e+01, Termination: CONVERGENCE\n",
      "2024-07-05 16:55:44,978 INFO: Removed outliers: 0\n",
      "2024-07-05 16:55:44,999 INFO: -------------------------------------------------------\n",
      "2024-07-05 16:55:45,014 INFO: image_url_pr_10_13_sample_13.jpg resection inliers: 756 / 756\n",
      "2024-07-05 16:55:45,032 INFO: Adding image_url_pr_10_13_sample_13.jpg to the reconstruction\n",
      "2024-07-05 16:55:45,142 INFO: Re-triangulating\n",
      "2024-07-05 16:55:45,144 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 16:55:45,144 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 16:55:45,144 INFO: GPS constraints X (3) - Xp (3)\n",
      "2024-07-05 16:55:46,357 INFO: Ran GLOBAL bundle in 1.18 secs.with 3/2478/5737 (2.32) shots/points/proj. (avg. length)\n",
      "2024-07-05 16:55:46,357 DEBUG: Ceres Solver Report: Iterations: 72, Initial cost: 1.582625e+02, Final cost: 5.575295e+01, Termination: CONVERGENCE\n",
      "2024-07-05 16:55:46,812 INFO: Ran GLOBAL bundle in 0.18 secs.with 3/2452/5685 (2.32) shots/points/proj. (avg. length)\n",
      "2024-07-05 16:55:46,813 DEBUG: Ceres Solver Report: Iterations: 10, Initial cost: 5.624335e+01, Final cost: 5.569709e+01, Termination: CONVERGENCE\n",
      "2024-07-05 16:55:46,835 INFO: Removed outliers: 0\n",
      "2024-07-05 16:55:46,835 INFO: Reconstruction now has 3 shots.\n",
      "2024-07-05 16:55:46,837 INFO: -------------------------------------------------------\n",
      "2024-07-05 16:55:46,840 INFO: Some images can not be added\n",
      "2024-07-05 16:55:46,840 INFO: -------------------------------------------------------\n",
      "2024-07-05 16:55:46,841 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 16:55:46,841 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 16:55:46,841 INFO: GPS constraints X (3) - Xp (3)\n",
      "2024-07-05 16:55:47,378 INFO: Ran GLOBAL bundle in 0.50 secs.with 3/2452/5685 (2.32) shots/points/proj. (avg. length)\n",
      "2024-07-05 16:55:47,378 DEBUG: Ceres Solver Report: Iterations: 31, Initial cost: 6.403691e+01, Final cost: 5.612600e+01, Termination: CONVERGENCE\n",
      "2024-07-05 16:55:47,400 INFO: Removed outliers: 0\n",
      "2024-07-05 16:55:47,429 INFO: Starting reconstruction with image_url_pr_10_13_sample_07.jpg and image_url_pr_10_13_sample_08.jpg\n",
      "2024-07-05 16:55:47,485 INFO: Two-view 5-points reconstruction inliers (transposed=False): 670 / 670\n",
      "2024-07-05 16:55:47,500 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 16:55:47,501 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 16:55:47,501 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 16:55:47,578 INFO: Triangulated: 670\n",
      "2024-07-05 16:55:47,662 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 16:55:47,663 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 16:55:47,663 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 16:55:47,800 INFO: Ran GLOBAL bundle in 0.13 secs.with 2/670/1340 (2.00) shots/points/proj. (avg. length)\n",
      "2024-07-05 16:55:47,800 DEBUG: Ceres Solver Report: Iterations: 29, Initial cost: 2.468867e+01, Final cost: 1.127197e+01, Termination: CONVERGENCE\n",
      "2024-07-05 16:55:47,805 INFO: Removed outliers: 0\n",
      "2024-07-05 16:55:47,814 INFO: -------------------------------------------------------\n",
      "2024-07-05 16:55:47,815 INFO: Collecting alignment constraints - bundle_use_gps:True bundle_use_gcp: True\n",
      "2024-07-05 16:55:47,815 INFO: GCP constraints X (0) - Xp (0)\n",
      "2024-07-05 16:55:47,815 INFO: GPS constraints X (2) - Xp (2)\n",
      "2024-07-05 16:55:47,890 INFO: Ran GLOBAL bundle in 0.07 secs.with 2/670/1340 (2.00) shots/points/proj. (avg. length)\n",
      "2024-07-05 16:55:47,891 DEBUG: Ceres Solver Report: Iterations: 14, Initial cost: 1.786056e+01, Final cost: 1.127198e+01, Termination: CONVERGENCE\n",
      "2024-07-05 16:55:47,897 INFO: Removed outliers: 0\n",
      "2024-07-05 16:55:47,907 INFO: Reconstruction 0: 3 images, 2452 points\n",
      "2024-07-05 16:55:47,907 INFO: Reconstruction 1: 2 images, 670 points\n",
      "2024-07-05 16:55:47,907 INFO: 2 partial reconstructions in total.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /opt/conda/bin/activate opensfm\n",
    "export PATH=$PATH:/opt/utils/OpenSfM/bin\n",
    "export MPLBACKEND=gtk3agg # if running within a notebook\n",
    "\n",
    "\n",
    "# This creates \"tracks\" for the features. That is to say, if a feature in image 1 is matched with one in image 2,\n",
    "# and in turn that one is matched with one in image 3, then it links the matches between 1 and 3. \n",
    "opensfm create_tracks CAP_sample_1\n",
    "\n",
    "# Calculates the essential matrix, the camera pose and the reconstructed feature points\n",
    "opensfm reconstruct CAP_sample_1\n",
    "\n",
    "# adding the --all command to include all partial reconstructions\n",
    "opensfm export_ply CAP_sample_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georegistration\n",
    "\n",
    "The process of assigning GPS coordinates to individual pixels is called _georegistration_ or _georeferencing_. This requires us to perform a final transformation from pixel coordinates *per each image* to the 3D reconstructed coordinates. Before doing so, it is worthwhile talking a bit about what exactly our 3D coordinate system is. \n",
    "\n",
    "You might recall that not all coordinate referece systems lend themselves well to geometric transformations. Specifically, we want our 3D coordinate system to be Cartesian (i.e. three orthogonal, right-handed axes). OpenSfM performs its reconstructions in what is known as a *local tangent plane coordinate system* called *local east, north, up (ENU) coordinates*. The way this works is, you select an origin somewhere in the world (in our case, it is saved in the reference_lla.json file), and you align your axes such that the x-axis is parallel to latitudes and increasing Eastward, the y-axis is parallel to meridians and increasing Northward, and the z-axis is pointing away from the center of the Earth. The image below shows how this works:\n",
    "\n",
    "<img src=\"notebook_images/enu.png\" width=\"500\"  />\n",
    "\n",
    "In order to convert from ENU coordinates to geodetic coordinates (i.e. latitude, longitude, altitude), you need to know the origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Origin of our reconstruction, as given by the reference_lla.json (made from the reconstruction)\n",
    "with open(\"CAP_sample_1/reference_lla.json\", \"r\") as f:\n",
    "    reference_lla = json.load(f)\n",
    "    latitude=reference_lla[\"latitude\"]\n",
    "    longitude=reference_lla[\"longitude\"]\n",
    "    altitude=reference_lla[\"altitude\"]\n",
    "\n",
    "# This is the json file that contains the reconstructed feature points\n",
    "with open(\"CAP_sample_1/reconstruction.json\", \"r\") as f:\n",
    "    reconstructions = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bit of work we need to go through to finalize the georegistration. First, we need to match the reconstructed features with the features on an image the tracks.csv file and the reconstruction.json can help us do that. The columns of tracks are as follows: image name, track ID (ID of the reconstructed point), feature ID (ID of the feature within the image), the *normalized* image coordinates x and y, the normalization factor s, and the color of the feature RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pybundle' from partially initialized module 'opensfm' (most likely due to a circular import) (/opt/utils/OpenSfM/opensfm/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/opt/utils/OpenSfM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopensfm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m denormalized_image_coordinates\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# reading the csv\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tracks \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAP_sample_1/tracks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/utils/OpenSfM/opensfm/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyre-unsafe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopensfm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pybundle\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopensfm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pydense\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopensfm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyfeatures\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pybundle' from partially initialized module 'opensfm' (most likely due to a circular import) (/opt/utils/OpenSfM/opensfm/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/utils/OpenSfM')\n",
    "\n",
    "\n",
    "from opensfm.features import denormalized_image_coordinates\n",
    "\n",
    "# reading the csv\n",
    "tracks = pd.read_csv(\"CAP_sample_1/tracks.csv\", sep=\"\\t\", skiprows=1, names=[\"image_name\", \"track_id\", \"feature_id\", \"x\", \"y\", \"s\", \"R\", \"G\", \"B\"])\n",
    "\n",
    "# we need to denormalize the coordinates to turn them into regular pixel coordinates\n",
    "normalized_coor = tracks[[\"x\", \"y\", \"s\"]]\n",
    "denormalized_coor = denormalized_image_coordinates(normalized_coor.values, 4496, 3000)\n",
    "\n",
    "# create a new column with the denormalized coordinates\n",
    "tracks[\"denorm_x\"] = denormalized_coor[:, 0]\n",
    "tracks[\"denorm_y\"] = denormalized_coor[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to store the georegistration by creating a new .tif file for every CAP image. As you can recall, .tif files save not just the pixel data but also the projection that allows it to be displayed on top of other map data. There are two parts to doing this:\n",
    "- First, we need to create an _orthorectified_ image. Simply put, this is one that is transformed such that it looks as though you are looking at it from the top down. \n",
    "- Second, we need to add *ground control points* (GCPs) to the orthorectified image. GCPs are correspondences between world coordinates and pixel coordinates.\n",
    "\n",
    "Once we add the GCPs, any mapping software can plot the image such that the GCPs are aligned with their underlying coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reconstructions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAP_sample_1/ortho/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAP_sample_1/ortho/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reconst \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreconstructions\u001b[49m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shot \u001b[38;5;129;01min\u001b[39;00m reconst[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshots\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# some housekeeping\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         shot_name \u001b[38;5;241m=\u001b[39m shot\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstructions' is not defined"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "from osgeo import osr\n",
    "try:\n",
    "    from pymap3d import enu2geodetic\n",
    "except:\n",
    "    !pip install pymap3d\n",
    "    from pymap3d import enu2geodetic\n",
    "\n",
    "import random\n",
    "from skimage import transform\n",
    "\n",
    "if not os.path.isdir(\"CAP_sample_1/geotiff/\"):\n",
    "    os.mkdir(\"CAP_sample_1/geotiff/\")\n",
    "if not os.path.isdir(\"CAP_sample_1/ortho/\"):\n",
    "    os.mkdir(\"CAP_sample_1/ortho/\")\n",
    "\n",
    "for reconst in reconstructions:\n",
    "    for shot in reconst[\"shots\"]:\n",
    "        # some housekeeping\n",
    "        shot_name = shot.split(\".\")[0]\n",
    "        img = cv2.imread(\"CAP_sample_1/images/\"+shot)\n",
    "        shape = img.shape\n",
    "        \n",
    "        # here we get the features from the image and their corresponding reconstructed features\n",
    "        reconst_ids = list(map(int, reconst[\"points\"].keys()))\n",
    "        tracks_shot = tracks[(tracks[\"image_name\"] == shot) & (tracks[\"track_id\"].isin(reconst_ids))]\n",
    "        denorm_shot = np.round(tracks_shot[[\"denorm_x\", \"denorm_y\"]].values)\n",
    "        reconst_shot = np.array([reconst[\"points\"][str(point)][\"coordinates\"] for point in tracks_shot[\"track_id\"]])\n",
    "        \n",
    "        # we're going to create an image that is distorted to fit within the world coordinates\n",
    "        # pix_shot is just the reconstructed feature coordinates offset by some amount so that\n",
    "        # all coordinates are positive.\n",
    "        offset = np.min(reconst_shot[:, :2])\n",
    "        pix_shot = reconst_shot[:, :2]-np.multiply(offset, offset<0)\n",
    "        \n",
    "        # transformation for the new orthorectified image\n",
    "        H, inliers = cv2.findHomography(denorm_shot, pix_shot)\n",
    "        \n",
    "        # filtering out points that didn't fit the transformation\n",
    "        reconst_shot = reconst_shot[inliers.ravel()==1, :]\n",
    "        denorm_shot = np.round(denorm_shot[inliers.ravel()==1, :])\n",
    "        pix_shot = np.round(pix_shot[inliers.ravel()==1, :])\n",
    "        \n",
    "        # creating the ortho image\n",
    "        shape = tuple(np.max(pix_shot, axis=0).astype(int))\n",
    "        ortho_img = cv2.warpPerspective(img, H, shape)\n",
    "        cv2.imwrite(\"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\", ortho_img)\n",
    "        \n",
    "        # here we convert all of the reconstructed points into lat/lon coordinates\n",
    "        geo_shot = np.array([enu2geodetic(reconst_shot[i, 0],reconst_shot[i, 1],reconst_shot[i, 2],latitude,longitude,altitude) for i in range(reconst_shot.shape[0])])        \n",
    "        \n",
    "        idx = random.sample(range(len(geo_shot)), 10)\n",
    "        pix_shot_sample = pix_shot[idx, :]\n",
    "        geo_shot_sample = geo_shot[idx, :]\n",
    "                \n",
    "        # creating the Ground Control Points\n",
    "        orig_fn = \"CAP_sample_1/ortho/\" + shot + \"_ortho.jpg\"\n",
    "        fn = \"CAP_sample_1/geotiff/\" + shot_name + \"_GCP.tif\"\n",
    "        \n",
    "        orig_ds = gdal.Open(orig_fn)\n",
    "        gdal.GetDriverByName('GTiff').CreateCopy(fn, orig_ds)\n",
    "        ds = gdal.Open(fn, gdal.GA_Update)\n",
    "        sr = osr.SpatialReference()\n",
    "        sr.SetWellKnownGeogCS('WGS84')\n",
    "        \n",
    "        gcps = [gdal.GCP(geo_shot_sample[i, 1], geo_shot_sample[i, 0], 0, int(pix_shot_sample[i, 0]), int(pix_shot_sample[i, 1])) for i in range(geo_shot_sample.shape[0])]\n",
    "        \n",
    "        ds.SetGCPs(gcps, sr.ExportToWkt())\n",
    "        \n",
    "        ds = None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "files = [os.path.join('CAP_sample_1/geotiff/', f) for f in os.listdir('CAP_sample_1') if f.endswith(\"tif\")]\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    with rasterio.open(file, \"r\") as dataset:\n",
    "#         dataset_mask = dataset.read_masks(1)\n",
    "#         dataset_read = dataset.read(1)\n",
    "#         rasterio.plot.show(np.ma.masked_where(dataset_mask==0, dataset_read), ax=ax)\n",
    "        ax = fig.add_subplot(3, 2, i+1)\n",
    "        rasterio.plot.show(dataset, ax=ax)\n",
    "        ax.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In the lesson folder, there is a spreadsheet with CAP images and their coordinates taken on October 13th, 2017. \n",
    "- Use geopandas to visualize the coordinates of all the images, and overlay it with some basemap\n",
    "- Select an area of those images that looks interesting to you. Use SfM to reconstruct at least 10 images\n",
    "- For those 10 images, select at least one and go through the georegistration process. Does the georegistration process yield good alignment with the ground truth? If not, why do you think that is?\n",
    "\n",
    "I **strongly** encourage you to tackle this as a team! Feel free to divide the tasks up as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 4: /path/to/reconstructed_image.tif: No such file or directory\n",
      "ERROR 4: /path/to/ground_truth.tif: No such file or directory\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'GetProjection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m gt_ds \u001b[38;5;241m=\u001b[39m gdal\u001b[38;5;241m.\u001b[39mOpen(ground_truth_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Georegister the image\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# This step involves manual or automated control point selection and transformation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# The following is a simplified conceptual example\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Transform the image to match the ground truth\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m gdal\u001b[38;5;241m.\u001b[39mWarp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/georegistered_image.tif\u001b[39m\u001b[38;5;124m'\u001b[39m, image_ds, dstSRS\u001b[38;5;241m=\u001b[39m\u001b[43mgt_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetProjection\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'GetProjection'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
